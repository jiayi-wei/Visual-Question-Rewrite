{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import tokenizer\n",
    "import data_preprocess\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./drive/My Drive/question_rewrite\"\n",
    "\n",
    "exp_name = \"res50_gen_token\"\n",
    "\n",
    "cate = \"auto_annot\"\n",
    "img_model_name = 'res50'\n",
    "fc_top = False\n",
    "\n",
    "# q_data longer, new_q_data shorter\n",
    "q_data, new_q_data, img_data = data_preprocess.read_data(cate)\n",
    "\n",
    "img_data = data_preprocess.extract_img_feat(cate,\n",
    "                                            img_data,\n",
    "                                            name=img_model_name,\n",
    "                                            fc=fc_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids, gen_tokenizer = tokenizer.general_preprocess(q_data)\n",
    "input_ids, gen_tokenizer = tokenizer.general_preprocess(new_q_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, val_input_ids = data_preprocess.train_val_split(input_ids)\n",
    "# train_input_masks, val_input_masks = data_preprocess.train_val_split(input_masks)\n",
    "# train_input_segments, val_input_segments = data_preprocess.train_val_split(input_segments)\n",
    "\n",
    "train_target_ids, val_target_ids = data_preprocess.train_val_split(target_ids)\n",
    "\n",
    "train_img, val_img = data_preprocess.train_val_split(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = len(train_input_ids)\n",
    "batch_size = 64\n",
    "steps_per_epoch = len(train_input_ids) // batch_size\n",
    "vocab_tar_size = len(gen_tokenizer.word_index) + 1\n",
    "embedding_dim = 256\n",
    "units = 768\n",
    "\n",
    "# bert input encoding\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((train_input_ids,\n",
    "#                                              train_input_masks,\n",
    "#                                              train_input_segments,\n",
    "#                                              train_target_ids,\n",
    "#                                              train_img))\n",
    "# dataset = dataset.map(lambda ids, masks, segs, targ, img:\n",
    "#                          tf.numpy_function(load_func,\n",
    "#                                            [ids, masks, segs, targ, img],\n",
    "#                                            [tf.int32, tf.int32, tf.int32, tf.int32, tf.float32]),\n",
    "#                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# general tokenize input and output\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_input_ids,\n",
    "                                              train_target_ids,\n",
    "                                              train_img))\n",
    "dataset = dataset.map(lambda ids, targ, img:\n",
    "                          tf.numpy_function(load_func,\n",
    "                                            [ids, targ, img],\n",
    "                                            [tf.int32, tf.int32, tf.float32]),\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = model.QRewriteModel(vocab_tar_size,\n",
    "                              embedding_dim,\n",
    "                              units,\n",
    "                              batch_size)\n",
    "# encoder = model.bert_encoder(input_max_length)\n",
    "encoder = model.GenEnocder(vocab_tar_size,\n",
    "                           embedding_dim,\n",
    "                           units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_input_ids, example_mask, example_seg, example_output_ids, example_img = next(iter(dataset))\n",
    "example_input_ids, example_mask, example_seg, example_output_ids, example_img = next(iter(dataset))\n",
    "print(\"input_ids:\", example_input_ids.shape)\n",
    "# print(\"mask:\", example_mask.shape)\n",
    "# print(\"segments:\", example_seg.shape)\n",
    "print(\"output_ids:\", example_output_ids.shape)\n",
    "print(\"img:\", example_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for input output both tf token\n",
    "\n",
    "@tf.function\n",
    "def train_step(ids, targ, img):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    dec_input = tf.expand_dims([target_tokenizer.word_index['[CLS]']] * batch_size, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output and image feature to the decoder\n",
    "      predictions, dec_hidden, _, _ = decoder(dec_input,\n",
    "                                              dec_hidden,\n",
    "                                              enc_output,\n",
    "                                              img)\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  \n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  \n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  return batch_loss\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  total_loss = 0\n",
    "\n",
    "  for batch, (ids, targ, img) in enumerate(dataset):\n",
    "    # enc_hidden, enc_output = encoder(ids, masks, segments)\n",
    "    batch_loss = train_step(ids, targ, img)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                 batch,\n",
    "                                                 batch_loss.numpy()))\n",
    "    \n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "  \n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
